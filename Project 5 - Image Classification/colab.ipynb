{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Applied-Machine-Learning-2022/project-5-jcar0-uark/blob/ahmed/colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFwKrxE38t9S"
      },
      "source": [
        "#### Copyright 2020 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpcrMDk48nqI"
      },
      "outputs": [],
      "source": [
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTZOeKjw8waH"
      },
      "source": [
        "# Image Classification Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pK5j0BXxrbfX"
      },
      "source": [
        "In this project we will build an image classification model and use the model to identify if the lungs pictured indicate that the patient has pneumonia. The outcome of the model will be true or false for each image.\n",
        "\n",
        "The [data is hosted on Kaggle](https://www.kaggle.com/rob717/pneumonia-dataset) and consists of 5,863 x-ray images. Each image is classified as 'pneumonia' or 'normal'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ht1GVr68swO"
      },
      "source": [
        "## Ethical Considerations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mW94_8-98vpR"
      },
      "source": [
        "We will frame the problem as:\n",
        "\n",
        "> *A hospital is having issues correctly diagnosing patients with pneumonia. Their current solution is to have two trained technicians examine every patient scan. Unfortunately, there are many times when two technicians are not available, and the scans have to wait for multiple days to be interpreted.*\n",
        ">\n",
        "> *They hope to fix this issue by creating a model that can identify if a patient has pneumonia. They will have one technician and the model both examine the scans and make a prediction. If the two agree, then the diagnosis is accepted. If the two disagree, then a second technician is brought in to provide their analysis and break the tie.*\n",
        "\n",
        "Discuss some of the ethical considerations of building and using this model. \n",
        "\n",
        "* Consider potential bias in the data that we have been provided. \n",
        "* Should this model err toward precision or accuracy?\n",
        "* What are the implications of massively over-classifying patients as having pneumonia?\n",
        "* What are the implications of massively under-classifying patients as having pneumonia?\n",
        "* Are there any concerns with having only one technician make the initial call?\n",
        "\n",
        "The questions above are prompts. Feel free to bring in other considerations you might have."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgUwTn_K-iK6"
      },
      "source": [
        "### **Student Solution**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider potential bias in the data that we have been provided."
      ],
      "metadata": {
        "id": "vhGD77mqlhd5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCud_ztplgN3"
      },
      "source": [
        "> Although the data being provided contains over 5,863 x-ray samples, it still may contain bias in some ways. In this dataset we are considering potential pneumonia patients, however out of the patients analyzed many features about the patient and their demographic are withheld. This could lead to a bias in the data since our model could potentially be trained with data unvaried in race, gender, or age. Another potential bias could be the quality of the images being used as well as the accuracy of the data being trained.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Should this model err toward precision or accuracy?"
      ],
      "metadata": {
        "id": "YmxDmjMPlh8B"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIKbYin5lgWr"
      },
      "source": [
        "> Precision and accuracy are both import when building and training a model. However, accuracy is vital with the goal of this model. Since, in the scenario our objective is to reduce the diagnosis from two-trained technicians to one, the model needs to pass the validation test. Only if it is accurate with one other trained technician will a diagnosis be complete. Our model could be precise and still be incorrect more times than it is correct."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What are the implications of massively over-classifying patients as having pneumonia?"
      ],
      "metadata": {
        "id": "Wg1kcLS8liRm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoWMY21ElgdV"
      },
      "source": [
        "> Over-classifying, or under-classifying patients can always become a problem. In this example, over-classifying could lead to the model not aligning with a professionals opinion, therefor leading to the patient having to go through additional examinations because of inconsistent diagnosis. An over-classification would also lead to a high congestion of patients making it harder for patients actually sick to schedule examinations."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What are the implications of massively under-classifying patients as having pneumonia?"
      ],
      "metadata": {
        "id": "4J21GEkqlirM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGjPQyRNlgxq"
      },
      "source": [
        "> The implications of underclassifying a patient would mean that we would have false positives,which could be fatal if the doctor prescribes the wrong medicine to the wrong patient. It would also mean that doctors would have to personally identify if a patient is positive or negative with pneumonia, which would waste time. This would lead to over crowded hospitals and an increase in mortality rate. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Are there any concerns with having only one technician make the initial call?"
      ],
      "metadata": {
        "id": "g8fuBZ4glwes"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0i0zCRDT-j58"
      },
      "source": [
        "> The problem with only having one technician making the intital call would be that it could be made for the wrong reasons if they do not consult with other technicians. This could lead to either resources being sent to places not needed where they could have been used elsewhere. That is why it is important for technicians to review each other's calls to see if it is needed or not."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9AxwuxE-nQt"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZFanABOAoHl"
      },
      "source": [
        "## Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxnS0ZlQAqNj"
      },
      "source": [
        "In this section of the lab, you will build, train, test, and validate a model or models. The data is the [\"Detecting Pneumonia\" dataset](https://www.kaggle.com/rob717/pneumonia-dataset). You will build a binary classifier that determines if an x-ray image has pneumonia or not.\n",
        "\n",
        "You'll need to:\n",
        "\n",
        "* Download the dataset\n",
        "* Perform EDA on the dataset\n",
        "* Build a model that can classify the data\n",
        "* Train the model using the training portion of the dataset. (It is already split out.)\n",
        "* Test at least three different models or model configurations using the testing portion of the dataset. This step can include changing model types, adding and removing layers or nodes from a neural network, or any other parameter tuning that you find potentially useful. Score the model (using accuracy, precision, recall, F1, or some other relevant score(s)) for each configuration.\n",
        "* After finding the \"best\" model and parameters, use the validation portion of the dataset to perform one final sanity check by scoring the model once more with the hold-out data.\n",
        "* If you train a neural network (or other model that you can get epoch-per-epoch performance), graph that performance over each epoch.\n",
        "\n",
        "Explain your work!\n",
        "\n",
        "> *Note: You'll likely want to [enable GPU in this lab](https://colab.research.google.com/notebooks/gpu.ipynb) if it is not already enabled.*\n",
        "\n",
        "If you get to a working solution you're happy with and want another challenge, you'll find pre-trained models on the [landing page of the dataset](https://www.kaggle.com/paultimothymooney/detecting-pneumonia-in-x-ray-images). Try to load one of those and see how it compares to your best model.\n",
        "\n",
        "Use as many text and code cells as you need to for your solution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XM35vYWSbim"
      },
      "source": [
        "### **Student Solution**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Download the dataset\n"
      ],
      "metadata": {
        "id": "7IDCYgYKaRgc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import os"
      ],
      "metadata": {
        "id": "vSoGmSx0KaoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivTzfzQN5jDk"
      },
      "outputs": [],
      "source": [
        "# Download the dataset\n",
        "! chmod 600 kaggle.json && (ls ~/.kaggle 2>/dev/null || mkdir ~/.kaggle) && cp kaggle.json ~/.kaggle/ && echo 'Done'\n",
        "! kaggle datasets download paultimothymooney/chest-xray-pneumonia\n",
        "! unzip chest-xray-pneumonia.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Explore some of the images in both classes\n",
        "\n",
        "train_images = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    'chest_xray/train/',\n",
        "    batch_size = 32\n",
        ")\n",
        "\n",
        "classes = train_images.class_names\n",
        "plt.figure(figsize = (10, 10))\n",
        "for images, labels in train_images.take(1):\n",
        "    for i in range(12):\n",
        "        plt.subplot(3, 4, i + 1)\n",
        "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "        plt.title(classes[labels[i]])\n",
        "        plt.axis(False)"
      ],
      "metadata": {
        "id": "KLTQRD-Wexwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train and Test our Models"
      ],
      "metadata": {
        "id": "hflMKymkl-jb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_datagen = ImageDataGenerator(rescale = 1./255, shear_range = 0.2, zoom_range = 0.2, horizontal_flip = True)\n",
        "\n",
        "test_val_datagen = ImageDataGenerator(rescale = 1./255)  #Image normalization.\n",
        "\n",
        "# load and iterate Training dataset\n",
        "train_it = train_datagen.flow_from_directory('chest_xray/train/', class_mode='binary', batch_size=32, shuffle = True, target_size=(64, 64))\n",
        "\n",
        "# load and iterate Testing dataset\n",
        "test_it = test_val_datagen.flow_from_directory('chest_xray/test/', class_mode='binary', batch_size=32, target_size=(64, 64))\n",
        "\n",
        "# load and iterate Validation dataset\n",
        "val_it = test_val_datagen.flow_from_directory('chest_xray/val/', class_mode='binary', batch_size=32, target_size=(64, 64))"
      ],
      "metadata": {
        "id": "5tn6Ln3NG2kB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Model 1"
      ],
      "metadata": {
        "id": "6ccZQY7_IOrt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CNN_model1 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv2D(16, 3, padding='same', activation='sigmoid',\n",
        "                           input_shape=(64, 64, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(),\n",
        "    tf.keras.layers.Conv2D(32, 3, padding='same', activation='sigmoid'),\n",
        "    tf.keras.layers.MaxPooling2D(),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(512, activation='sigmoid'),\n",
        "    tf.keras.layers.Dense(1, activation='softmax')\n",
        "])\n",
        "\n",
        "CNN_model1.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Training\n",
        "history1 = CNN_model1.fit(\n",
        "    train_it,\n",
        "    steps_per_epoch = 163,\n",
        "    epochs = 5,\n",
        ")\n",
        "\n",
        "# Scoring\n",
        "print(\"Model 1 Testing:\")\n",
        "CNN_model1.evaluate(test_it)"
      ],
      "metadata": {
        "id": "yeoDJpDEl28U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot Accuracy & Loss across Epochs\n",
        "\n",
        "plt.plot(history1.history['accuracy'])\n",
        "plt.plot(history1.history['loss'])\n",
        "plt.title('Training Accuracy & Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Accuracy', 'Loss'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qDxQ1nYaTCjM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Model 2"
      ],
      "metadata": {
        "id": "Vn66w1pnIPOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CNN_model2 =  tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\", input_shape=(64, 64, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(pool_size = (2, 2)),\n",
        "    tf.keras.layers.MaxPooling2D(pool_size = (2, 2)),\n",
        "    tf.keras.layers.Conv2D(32, (3, 3), activation=\"sigmoid\"),\n",
        "    tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\"),\n",
        "    tf.keras.layers.MaxPooling2D(pool_size = (2, 2)),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(activation = 'relu', units = 128),\n",
        "    tf.keras.layers.Dense(activation = 'sigmoid', units = 1)\n",
        "])\n",
        "\n",
        "CNN_model2.compile(optimizer = 'adam', \n",
        "               loss = 'binary_crossentropy', \n",
        "               metrics = ['accuracy'])\n",
        "\n",
        "# Training\n",
        "history2 = CNN_model2.fit(\n",
        "    train_it,\n",
        "    steps_per_epoch = 163, \n",
        "    epochs = 10, \n",
        ")\n",
        "\n",
        "# Scoring \n",
        "print(\"Model 2 Testing:\")\n",
        "CNN_model2.evaluate(test_it)"
      ],
      "metadata": {
        "id": "0opgGBPYmkSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot Accuracy & Loss across Epochs\n",
        "\n",
        "plt.plot(history2.history['accuracy'])\n",
        "plt.plot(history2.history['loss'])\n",
        "plt.title('Training Accuracy & Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Accuracy', 'Loss'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "q27oZSyoTGTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Model 3"
      ],
      "metadata": {
        "id": "ax4N4d4uIO8U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression Model\n",
        "\n",
        "X_train = []\n",
        "X_test = []\n",
        "\n",
        "for i in range(len(train_it[0][0])):\n",
        "  X_train.append(train_it[0][0][i].flatten())\n",
        "\n",
        "for i in range(len(test_it[0][0])):\n",
        "  X_test.append(test_it[0][0][i].flatten())\n",
        "\n",
        "y_train = np.array(train_it[0][1])\n",
        "y_test = np.array(test_it[0][1])\n",
        "\n",
        "# Training\n",
        "model3 = LogisticRegression(random_state=0, max_iter=1000).fit(X_train, y_train)\n",
        "\n",
        "# Scoring\n",
        "model3.score(X_test,y_test)"
      ],
      "metadata": {
        "id": "b8gEKk4TmkEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Validate our best Model"
      ],
      "metadata": {
        "id": "Qn-F6MnNadP9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use our validation data to check our model\n",
        "\n",
        "CNN_model2.evaluate(val_it)"
      ],
      "metadata": {
        "id": "8PVrJWDTqCN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5AaFcUV8NCB"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "yFwKrxE38t9S",
        "w5j2HpIW-ocU",
        "GSsiZ2YB1ADy"
      ],
      "name": "colab.ipynb",
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}